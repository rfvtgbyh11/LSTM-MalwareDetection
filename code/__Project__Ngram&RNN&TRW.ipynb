{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Data... 96 malware parsed\n",
      "Making Data... 194 malware parsed\n",
      "Making Data... 286 malware parsed\n",
      "Making Data... 380 malware parsed\n",
      "Making Data... 477 malware parsed\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\mute\\\\Desktop\\\\malware_api\\\\595.txt'\n",
      "malware 595 is not founded.\n",
      "Making Data... 660 malware parsed\n",
      "Making Data... 756 malware parsed\n",
      "Making Data... 853 malware parsed\n",
      "Making Data... 950 malware parsed\n",
      "Making Data... 1046 malware parsed\n",
      "Making Data... 1142 malware parsed\n",
      "Making Data... 1239 malware parsed\n",
      "Making Data... 1330 malware parsed\n",
      "Making Data... 1526 malware parsed\n",
      "Making Data... 1622 malware parsed\n",
      "Making Data... 1715 malware parsed\n",
      "Making Data... 1815 malware parsed\n",
      "Making Data... 1905 malware parsed\n",
      "Making Data... 2003 malware parsed\n",
      "Making Data... 2098 malware parsed\n",
      "Making Data... 73 benign parsed\n",
      "Making Data... 150 benign parsed\n",
      "Making Data... 232 benign parsed\n",
      "Making Data... 387 benign parsed\n",
      "Making Data... 465 benign parsed\n",
      "Making Data... 537 benign parsed\n",
      "Making Data... 617 benign parsed\n",
      "Making Data... 695 benign parsed\n",
      "Making Data... 772 benign parsed\n",
      "Making Data... 842 benign parsed\n",
      "Making Data... 912 benign parsed\n",
      "Making Data... 1138 benign parsed\n",
      "Making Data... 1214 benign parsed\n",
      "Making Data... 1361 benign parsed\n",
      "Making Data... 1442 benign parsed\n",
      "Making Data... 1596 benign parsed\n",
      "Making Completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p1 = re.compile(\"New process: .*\")\n",
    "p2 = re.compile(\"Number of api: [\\d]*\")\n",
    "p3 = re.compile(\"Number of process : [\\d]*\")\n",
    "\n",
    "api_list = set()\n",
    "api_sequence = {\"malware\" : [], \"benign\" : []}\n",
    "\n",
    "for kind in [\"malware\", \"benign\"]:\n",
    "    number_of_program = -1\n",
    "    for i in range(1, 2250):\n",
    "        try:\n",
    "            with open(r\"C:\\Users\\mute\\Desktop\\{}_api\\{}.txt\".format(kind, i), \"r\") as f:\n",
    "                txt = f.read()\n",
    "            temp = txt.split(\"\\n\")\n",
    "            if p3.match(temp[1]):\n",
    "                continue\n",
    "            \n",
    "            api_sequence[kind].append([])\n",
    "            number_of_program += 1\n",
    "            \n",
    "            number_of_process = -1\n",
    "            for line in temp:\n",
    "                if p1.match(line):\n",
    "                    api_sequence[kind][number_of_program].append([])\n",
    "                    number_of_process += 1\n",
    "                elif not p2.match(line) and not p3.match(line) and not \"\":\n",
    "                    api_sequence[kind][number_of_program][number_of_process].append(line)\n",
    "                    api_list.add(line)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"{} {} is not founded.\".format(kind, i))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Making Data... {} {} parsed\".format(len(api_sequence[kind]), kind))\n",
    "\n",
    "print(\"Making Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269 274 269 269 269 269 197 197 197  41 194   9 258 151 151 271 245 244\n",
      " 213   9   9 194 258 151 151 250 109 109  79  72 234 234 157 157  72 234\n",
      "  79  72 234  79  72 234  79 258 151  79 237   9   9   9]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([api2idx[i] for i in api_sequence[\"malware\"][0][0]])\n",
    "print(text_as_int[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "319600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dict()\n",
    "cnt = 0\n",
    "\n",
    "for program in api_sequence[\"malware\"][0:1600]:\n",
    "    cnt += 1\n",
    "    for process in program:\n",
    "        for i in range(len(process)-3):\n",
    "            ngram = ' '.join(process[i:i+4])\n",
    "            if ngram not in temp:\n",
    "                temp[ngram] = 1\n",
    "            else:\n",
    "                temp[ngram] += 1\n",
    "    if cnt %400 == 0:\n",
    "        print(\"continue...\")\n",
    "        \n",
    "for program in api_sequence[\"benign\"][0:1600]:\n",
    "    cnt += 1\n",
    "    for process in program:\n",
    "        for i in range(len(process)-3):\n",
    "            ngram = ' '.join(process[i:i+4])\n",
    "            if ngram not in temp:\n",
    "                temp[ngram] = 1\n",
    "            else:\n",
    "                temp[ngram] += 1\n",
    "    if cnt %400 == 0:\n",
    "        print(\"continue...\")\n",
    "    \n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NtAllocateVirtualMemory NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  304544\n",
      "NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  594613\n",
      "NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  280116\n",
      "NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory GetFileType :  2538\n",
      "NtAllocateVirtualMemory NtAllocateVirtualMemory GetFileType GetFileType :  2600\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in temp.keys():\n",
    "    print(i, \": \", temp[i])\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727\n"
     ]
    }
   ],
   "source": [
    "ngramNeedToDel = []\n",
    "\n",
    "for i in temp.keys():\n",
    "    if temp[i] < 5000:\n",
    "        ngramNeedToDel.append(i)\n",
    "        \n",
    "for i in ngramNeedToDel:\n",
    "    del temp[i]\n",
    "\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NtAllocateVirtualMemory NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  304544\n",
      "NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  594613\n",
      "NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  280116\n",
      "NtClose LdrLoadDll LdrGetProcedureAddress LdrGetProcedureAddress :  9821\n",
      "LdrLoadDll LdrGetProcedureAddress LdrGetProcedureAddress GetSystemWindowsDirectoryW :  8886\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in temp.keys():\n",
    "    print(i, \": \", temp[i])\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NtAllocateVirtualMemory NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  0\n",
      "NtFreeVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  1\n",
      "NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory NtAllocateVirtualMemory :  2\n",
      "NtClose LdrLoadDll LdrGetProcedureAddress LdrGetProcedureAddress :  3\n",
      "LdrLoadDll LdrGetProcedureAddress LdrGetProcedureAddress GetSystemWindowsDirectoryW :  4\n"
     ]
    }
   ],
   "source": [
    "ngram2idx = {}\n",
    "cnt = 0\n",
    "for i in temp.keys():\n",
    "    ngram2idx[i] = cnt\n",
    "    cnt+= 1\n",
    "\n",
    "cnt = 0\n",
    "for i in ngram2idx.keys():\n",
    "    print(i, \": \", ngram2idx[i])\n",
    "    cnt+= 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n",
      "continue...\n"
     ]
    }
   ],
   "source": [
    "data_malware = []\n",
    "idx = 0\n",
    "\n",
    "for program in api_sequence[\"malware\"][0:1600]:\n",
    "    data_malware.append([])\n",
    "    cnt = 0\n",
    "    \n",
    "    for process in program:\n",
    "        for i in range(len(process)-3):\n",
    "            ngram = ' '.join(process[cnt:cnt+4])\n",
    "            if ngram in ngram2idx.keys():\n",
    "                data_malware[idx].append(ngram2idx[ngram])\n",
    "                cnt += 3\n",
    "            cnt += 1\n",
    "        \n",
    "    idx += 1\n",
    "    if idx %400 == 0:\n",
    "        print(\"continue...\")\n",
    "\n",
    "data_benign = []\n",
    "idx = 0\n",
    "\n",
    "for program in api_sequence[\"benign\"][0:1600]:\n",
    "    data_benign.append([])\n",
    "    cnt = 0\n",
    "    \n",
    "    for process in program:\n",
    "        for i in range(len(process)-3):\n",
    "            ngram = ' '.join(process[cnt:cnt+4])\n",
    "            if ngram in ngram2idx.keys():\n",
    "                data_benign[idx].append(ngram2idx[ngram])\n",
    "                cnt += 3\n",
    "            cnt += 1\n",
    "                \n",
    "    idx += 1\n",
    "    if idx %400 == 0:\n",
    "        print(\"continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  3  7  10  14  15  16  20  24  3  "
     ]
    }
   ],
   "source": [
    "for i in data_malware[0][0:10]:\n",
    "    print(i, \" \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131000\n",
      "<BatchDataset shapes: (100,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "temp = []\n",
    "for program in data_malware:\n",
    "    try:\n",
    "        for i in range(30):\n",
    "            idx = randint(0, len(program)-100)\n",
    "            for j in range(100):\n",
    "                temp.append(program[idx+j])\n",
    "    except Exception:\n",
    "        for i in range(30):\n",
    "            cnt = 0\n",
    "            for j in program:\n",
    "                cnt += 1\n",
    "            if cnt < 50:\n",
    "                break\n",
    "            for j in range(100-cnt):\n",
    "                temp.append(0)\n",
    "            for j in program:\n",
    "                temp.append(j)\n",
    "\n",
    "print(len(temp))\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(temp)\n",
    "dataset1 = dataset1.batch(100, drop_remainder = True)\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(data):\n",
    "    return data, 1\n",
    "dataset1 = dataset1.map(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  7, 10, 14, 15, 16,\n",
      "       20, 24,  3, 13, 21, 25, 28, 29, 29, 29, 29, 30, 32, 32, 33, 37, 39,\n",
      "       13, 21, 25, 40, 10, 26, 22, 33, 37, 33, 37, 33, 46, 44, 36, 44, 45,\n",
      "       49, 33, 46, 50, 51, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "       53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 56, 56, 56, 57, 61])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset1.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131000\n",
      "<BatchDataset shapes: (100,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for program in data_benign:\n",
    "    try:\n",
    "        for i in range(30):\n",
    "            idx = randint(0, len(program)-100)\n",
    "            for j in range(100):\n",
    "                temp.append(program[idx+j])\n",
    "    except Exception:\n",
    "        for i in range(30):\n",
    "            cnt = 0\n",
    "            for j in program:\n",
    "                cnt += 1\n",
    "            if cnt < 50:\n",
    "                break\n",
    "            for j in range(100-cnt):\n",
    "                temp.append(0)\n",
    "            for j in program:\n",
    "                temp.append(j)\n",
    "    \n",
    "print(len(temp))\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(temp)\n",
    "dataset2 = dataset2.batch(100, drop_remainder = True)\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_0(data):\n",
    "    return data, 0\n",
    "\n",
    "dataset2 = dataset2.map(label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409,\n",
      "       171,   3,   7, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
      "       329, 232, 232, 232, 232, 232, 232, 232, 459, 329, 329, 329, 329,\n",
      "       329, 329, 329, 329, 232, 232, 232, 232, 232, 232, 232, 232, 232,\n",
      "       459, 329, 329, 329, 329, 329, 186, 329, 232, 232, 232, 232, 232,\n",
      "       232, 232, 459, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232,\n",
      "       232, 232, 232, 232, 460,  66, 186, 185, 180, 329, 329, 329, 329,\n",
      "       329, 329, 329, 329, 329, 329, 329, 329, 329])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset2.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: ((100,), ()), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset1.concatenate(dataset2)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset shapes: ((100,), ()), types: (tf.int32, tf.int32)>\n",
      "22620\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "cnt = 0\n",
    "for i in dataset:\n",
    "    cnt +=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = cnt\n",
    "TRAIN_SIZE = int(0.7 * DATASET_SIZE)\n",
    "TEST_SIZE = int(0.3 * DATASET_SIZE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = dataset.take(TRAIN_SIZE).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = dataset.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64,)), types: (tf.int32, tf.int32)>\n",
      "<SkipDataset shapes: ((100,), ()), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset(batch 100, seq_length 100)의 개수:  247\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in train_dataset:\n",
    "    cnt += 1\n",
    "\n",
    "print(\"train_dataset(batch 100, seq_length 100)의 개수: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_size = len(api_list)\n",
    "rnn_units = 32\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(api_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = models.Sequential([\n",
    "    layers.Embedding(api_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    layers.LSTM(rnn_units,\n",
    "                        return_sequences=False,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  api_size = len(ngram2idx),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 64)            46528     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, 32)                  12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 58,977\n",
      "Trainable params: 58,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1) (배치 사이즈, output 차원)\n",
      "훈련 전 확률출력값:  0.49977505\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "print(example_batch_predictions.shape, \"(배치 사이즈, output 차원)\")\n",
    "print(\"훈련 전 확률출력값: \", example_batch_predictions.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch의 10번째 output :  0.5029396\n",
      "batch의 20번째 output :  0.49282366\n",
      "batch의 30번째 output :  0.49645847\n",
      "batch의 40번째 output :  0.49243516\n",
      "batch의 50번째 output :  0.5003868\n",
      "batch의 60번째 output :  0.51433873\n",
      "batch의 10번째 label:  1\n",
      "batch의 20번째 label:  0\n",
      "batch의 30번째 label:  1\n",
      "batch의 40번째 label:  1\n",
      "batch의 50번째 label:  0\n",
      "batch의 60번째 label:  1\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for sampled_indices in example_batch_predictions:\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    cnt += 1\n",
    "    if cnt%10 == 0:\n",
    "        print(\"batch의 %d번째 output : \" % cnt, sampled_indices)\n",
    "\n",
    "cnt = 0\n",
    "for sampled_indices in target_example_batch:\n",
    "    cnt += 1\n",
    "    if cnt%10 == 0:\n",
    "        print(\"batch의 %d번째 label: \" % cnt, sampled_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction batch size(shape):  (64, 1)  # (batch size, 1)\n",
      "loss:  0.6930939\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.binary_crossentropy(\n",
    "    labels, logits, from_logits=False, label_smoothing=0\n",
    "    )\n",
    "\n",
    "print(\"prediction batch size(shape): \", example_batch_predictions.shape, \" # (batch size, 1)\")\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"loss: \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/training_checkpoints/Untitled1'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=[tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.4111\n",
      "Epoch 2/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.3184\n",
      "Epoch 3/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2972\n",
      "Epoch 4/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2792\n",
      "Epoch 5/25\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.2708\n",
      "Epoch 6/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2577\n",
      "Epoch 7/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2528\n",
      "Epoch 8/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2430: 0s -\n",
      "Epoch 9/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2386\n",
      "Epoch 10/25\n",
      "247/247 [==============================] - 6s 25ms/step - loss: 0.2312\n",
      "Epoch 11/25\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.2244\n",
      "Epoch 12/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2282\n",
      "Epoch 13/25\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.2214\n",
      "Epoch 14/25\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.2163\n",
      "Epoch 15/25\n",
      "247/247 [==============================] - 6s 24ms/step - loss: 0.2150\n",
      "Epoch 16/25\n",
      "247/247 [==============================] - 6s 25ms/step - loss: 0.2076\n",
      "Epoch 17/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2024\n",
      "Epoch 18/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.2072\n",
      "Epoch 19/25\n",
      "247/247 [==============================] - 6s 22ms/step - loss: 0.1964\n",
      "Epoch 20/25\n",
      "247/247 [==============================] - 6s 26ms/step - loss: 0.1942\n",
      "Epoch 21/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.1904\n",
      "Epoch 22/25\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.1864\n",
      "Epoch 23/25\n",
      "247/247 [==============================] - 5s 22ms/step - loss: 0.1861\n",
      "Epoch 24/25\n",
      "247/247 [==============================] - 6s 24ms/step - loss: 0.1807\n",
      "Epoch 25/25\n",
      "247/247 [==============================] - 6s 23ms/step - loss: 0.1835\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/training_checkpoints/Untitled1\\\\ckpt_25'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fcca338e08>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = build_model(\n",
    "  api_size = len(ngram2idx),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=1)\n",
    "\n",
    "test_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             46528     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 32)                   12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 1)                    33        \n",
      "=================================================================\n",
      "Total params: 58,977\n",
      "Trainable params: 58,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset의 수:  6787\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in test_dataset:\n",
    "    cnt += 1\n",
    "print(\"test dataset의 수: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 정확도:  0.974\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "\n",
    "cnt = 0\n",
    "pred = 0\n",
    "\n",
    "mal_dist_divided30=[]\n",
    "ben_dist_divided30=[]\n",
    "mal_temp=[]\n",
    "ben_temp=[]\n",
    "mal_avg_dist=[]\n",
    "ben_avg_dist=[]\n",
    "\n",
    "for data in test_dataset:\n",
    "    input_example = data[0]\n",
    "    input_example = tf.expand_dims(input_example, 0)\n",
    "    predictions = test_model(input_example)\n",
    "    predictions = tf.squeeze(predictions)\n",
    "        \n",
    "    target_example = data[1]\n",
    "    \n",
    "    if abs(target_example.numpy() - predictions.numpy() < 0.5):\n",
    "        pred += 1\n",
    "    cnt += 1\n",
    "    \n",
    "    if target_example.numpy() == 1:\n",
    "        mal_temp.append(predictions.numpy())\n",
    "        if len(mal_temp) == 30:\n",
    "            mal_dist_divided30.append(mal_temp)\n",
    "            mal_temp=[]\n",
    "    else:\n",
    "        ben_temp.append(predictions.numpy())\n",
    "        if len(ben_temp) == 30:\n",
    "            ben_dist_divided30.append(ben_temp)\n",
    "            ben_temp=[]\n",
    "\n",
    "print(\"훈련 정확도: \", round(pred/cnt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "귀무가설 H0의 평균:  0.109066665  표준편차:  0.039181937\n",
      "대립가설 H1의 평균:  0.8901029  표준편차:  0.033646565\n"
     ]
    }
   ],
   "source": [
    "for i in mal_dist_divided30:\n",
    "    mal_avg_dist.append(st.mean(i))\n",
    "\n",
    "H1_mean = np.mean(mal_avg_dist)\n",
    "H1_stdev = np.std(mal_avg_dist)\n",
    "    \n",
    "for i in ben_dist_divided30:\n",
    "    ben_avg_dist.append(st.mean(i))\n",
    "H0_mean = np.mean(ben_avg_dist)\n",
    "H0_stdev = np.std(ben_avg_dist)\n",
    "\n",
    "print(\"귀무가설 H0의 평균: \", H0_mean, \" 표준편차: \", H0_stdev)\n",
    "print(\"대립가설 H1의 평균: \", H1_mean, \" 표준편차: \", H1_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TooShortLengthException(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def TRW(test_model, test_data, mal_norm, ben_norm):\n",
    "    Lambda = 1\n",
    "    threshold = 1e15\n",
    "    temp=[]\n",
    "    cnt=0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            idx = randint(0, len(test_data)-100)\n",
    "            for j in range(100):\n",
    "                temp.append(test_data[idx+j])\n",
    "            \n",
    "            buf = tf.data.Dataset.from_tensor_slices(temp).batch(100)\n",
    "            for i in buf:\n",
    "                input_data = tf.expand_dims(i, 0)\n",
    "            predictions = test_model(input_data)\n",
    "            prob = tf.squeeze(predictions)\n",
    "                \n",
    "            ben_prob = 1 - ben_norm.cdf(prob.numpy())\n",
    "            mal_prob = mal_norm.cdf(prob.numpy())\n",
    "            Lambda *= (mal_prob / ben_prob * (2**cnt))\n",
    "            cnt += 1\n",
    "            \n",
    "            if Lambda > threshold:\n",
    "                return True\n",
    "            elif Lambda < 1/threshold:\n",
    "                return False\n",
    "            \n",
    "    except ValueError:\n",
    "        raise TooShortLengthException(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision:  0.961\n"
     ]
    }
   ],
   "source": [
    "ben_norm = norm(H0_mean, H0_stdev*5.48)\n",
    "mal_norm = norm(H1_mean, H1_stdev*5.48)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for test_data in data_benign[400:500]:\n",
    "    try:\n",
    "        rtn = TRW(test_model, test_data, mal_norm, ben_norm)\n",
    "        ans = False\n",
    "        \n",
    "        if rtn == ans:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    except TooShortLengthException as e:\n",
    "        continue\n",
    "    \n",
    "for test_data in data_malware[400:500]:\n",
    "    try:\n",
    "        rtn = TRW(test_model, test_data, mal_norm, ben_norm)\n",
    "        ans = True\n",
    "        \n",
    "        if rtn == ans:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    except TooShortLengthException as e:\n",
    "        continue\n",
    "    \n",
    "print(\"model precision: \", round(correct/(correct+wrong), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
